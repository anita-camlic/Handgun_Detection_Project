{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import os, shutil\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dir = 'C:/Users/User/Documents/Flatiron/Capstone/train_data/images'\n",
    "test_images_dir = 'C:/Users/User/Documents/Flatiron/Capstone/test_data/images'\n",
    "\n",
    "#train_images_dir_0 = 'C:/Users/User/Documents/Flatiron/Capstone/train_data/images/0'\n",
    "#train_images_dir_1 = 'C:/Users/User/Documents/Flatiron/Capstone/train_data/images/1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepping data for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Generator Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Validation set creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_target_size = (128,128)\n",
    "global_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3156 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Batch size here means how many \n",
    "#of the images do we want our model to be trained on\n",
    "train_val_generator = ImageDataGenerator(rescale=1./255,\n",
    "                                         validation_split = .10)\n",
    "train_data = train_val_generator.flow_from_directory(train_images_dir,  \n",
    "                                                     target_size = global_target_size,\n",
    "                                                     subset='training',\n",
    "                                                     batch_size=global_batch_size,\n",
    "                                                     class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 350 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data = train_val_generator.flow_from_directory(train_images_dir,\n",
    "       subset = 'validation',  \n",
    "       target_size = global_target_size,\n",
    "       batch_size=global_batch_size,\n",
    "       class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 809 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = ImageDataGenerator(rescale=1./255)\n",
    "test_data = test_generator.flow_from_directory(\n",
    "        test_images_dir,  \n",
    "        target_size = global_target_size,\n",
    "        batch_size=global_batch_size,\n",
    "        class_mode='binary',\n",
    "        shuffle= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Here I am creating a dictionary to store all of my scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dictionary = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: Running with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = models.Sequential()\n",
    "model1.add(layers.Conv2D(filters = 32,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu',\n",
    "                         input_shape = (global_target_size[0],global_target_size[0], 3)\n",
    "                        ))\n",
    "model1.add(layers.MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model1.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model1.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model1.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model1.add(layers.Flatten())\n",
    "model1.add(layers.Dense(64, activation='relu'))\n",
    "model1.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer = \"adam\",\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "99/99 [==============================] - 129s 1s/step - loss: 0.5560 - acc: 0.7113 - val_loss: 0.5279 - val_acc: 0.7057\n",
      "Epoch 2/15\n",
      "99/99 [==============================] - 128s 1s/step - loss: 0.3950 - acc: 0.8156 - val_loss: 0.6924 - val_acc: 0.7143\n",
      "Epoch 3/15\n",
      "99/99 [==============================] - 127s 1s/step - loss: 0.3334 - acc: 0.8470 - val_loss: 0.4614 - val_acc: 0.7971\n",
      "Epoch 4/15\n",
      "99/99 [==============================] - 128s 1s/step - loss: 0.2680 - acc: 0.8866 - val_loss: 0.5118 - val_acc: 0.7800\n",
      "Epoch 5/15\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.2301 - acc: 0.8989 - val_loss: 0.4742 - val_acc: 0.7771\n",
      "Epoch 6/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.1762 - acc: 0.9259 - val_loss: 0.5504 - val_acc: 0.7771\n",
      "Epoch 7/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.1333 - acc: 0.9449 - val_loss: 0.6906 - val_acc: 0.7629\n",
      "Epoch 8/15\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.0932 - acc: 0.9632 - val_loss: 0.7540 - val_acc: 0.7429\n",
      "Epoch 9/15\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.0706 - acc: 0.9740 - val_loss: 0.7416 - val_acc: 0.8171\n",
      "Epoch 10/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.0400 - acc: 0.9886 - val_loss: 0.8385 - val_acc: 0.7914\n",
      "Epoch 11/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.0326 - acc: 0.9895 - val_loss: 1.1304 - val_acc: 0.7771\n",
      "Epoch 12/15\n",
      "99/99 [==============================] - 127s 1s/step - loss: 0.0401 - acc: 0.9883 - val_loss: 1.0204 - val_acc: 0.7600\n",
      "Epoch 13/15\n",
      "99/99 [==============================] - 127s 1s/step - loss: 0.0445 - acc: 0.9835 - val_loss: 1.3295 - val_acc: 0.7371\n",
      "Epoch 14/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.0384 - acc: 0.9892 - val_loss: 1.1556 - val_acc: 0.7600\n",
      "Epoch 15/15\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.0325 - acc: 0.9895 - val_loss: 1.0365 - val_acc: 0.7914\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(train_data,\n",
    "                    epochs=15,\n",
    "                    validation_data = val_data\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 126, 126, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 60, 60, 32)        16416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                802880    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 838,753\n",
      "Trainable params: 838,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 82s 830ms/step - loss: 0.0239 - acc: 0.9911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.023903056979179382, 0.9911280274391174]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train_1 = model1.evaluate(train_data)\n",
    "results_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 8s 739ms/step - loss: 1.0365 - acc: 0.7914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.036498785018921, 0.7914285659790039]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_val_1 = model1.evaluate(val_data)\n",
    "results_val_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dictionary['model 1'] =  results_val_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN 1 Train: loss: 4.6420e-05 - acc: 1.0000\n",
    "CNN 1 Validation: loss: 5.2256 - acc: 0.5000\n",
    "Highly overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my goal with this next model is to reduce overfitting - I will attempt to do that by adding a dropout later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.Sequential()\n",
    "model2.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                         input_shape = (global_target_size[0],global_target_size[0], 3)\n",
    "                        ))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(64, activation='relu'))\n",
    "model2.add(layers.Dropout(0.5))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer = \"adam\",\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.5795 - acc: 0.6778 - val_loss: 0.5349 - val_acc: 0.7314\n",
      "Epoch 2/15\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.4549 - acc: 0.7817 - val_loss: 0.4550 - val_acc: 0.7629\n",
      "Epoch 3/15\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.3944 - acc: 0.8112 - val_loss: 0.4239 - val_acc: 0.8629\n",
      "Epoch 4/15\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.3348 - acc: 0.8492 - val_loss: 0.4538 - val_acc: 0.7571\n",
      "Epoch 5/15\n",
      "99/99 [==============================] - 127s 1s/step - loss: 0.2888 - acc: 0.8691 - val_loss: 0.4486 - val_acc: 0.8200\n",
      "Epoch 6/15\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.2462 - acc: 0.8939 - val_loss: 0.4167 - val_acc: 0.7971\n",
      "Epoch 7/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.2255 - acc: 0.9053 - val_loss: 0.5091 - val_acc: 0.7686\n",
      "Epoch 8/15\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.1836 - acc: 0.9243 - val_loss: 0.5924 - val_acc: 0.7200\n",
      "Epoch 9/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.1511 - acc: 0.9376 - val_loss: 0.6746 - val_acc: 0.7600\n",
      "Epoch 10/15\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.1329 - acc: 0.9436 - val_loss: 0.6281 - val_acc: 0.7371\n",
      "Epoch 11/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.1228 - acc: 0.9496 - val_loss: 0.6554 - val_acc: 0.7800\n",
      "Epoch 12/15\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.1120 - acc: 0.9560 - val_loss: 0.5577 - val_acc: 0.8114\n",
      "Epoch 13/15\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.0913 - acc: 0.9626 - val_loss: 0.7636 - val_acc: 0.7914\n",
      "Epoch 14/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.0670 - acc: 0.9718 - val_loss: 0.7541 - val_acc: 0.7943\n",
      "Epoch 15/15\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.0584 - acc: 0.9769 - val_loss: 0.8710 - val_acc: 0.7857\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(train_data,\n",
    "                    epochs=15,\n",
    "                    #batch_size = 5,\n",
    "                    validation_data = val_data\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 82s 827ms/step - loss: 0.0270 - acc: 0.9924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.027039606124162674, 0.9923954606056213]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train_2 = model2.evaluate(train_data)\n",
    "results_train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 8s 759ms/step - loss: 0.8710 - acc: 0.7857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8709557056427002, 0.7857142686843872]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_val_2 = model2.evaluate(val_data)\n",
    "results_val_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dictionary['model 2'] =  results_val_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the last model reduced overfitting a little bit, but now enough as I would like, I am going to add another dropout layer to hopefully reduce the overfitting more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = models.Sequential()\n",
    "model3.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                         input_shape = (global_target_size[0],global_target_size[0], 3)\n",
    "                        ))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model3.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model3.add(layers.Flatten())\n",
    "model3.add(layers.Dense(64, activation='relu'))\n",
    "model3.add(layers.Dropout(0.25))\n",
    "model3.add(layers.Dense(64, activation='relu'))\n",
    "model3.add(layers.Dropout(0.25))\n",
    "model3.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.compile(loss='binary_crossentropy',\n",
    "              optimizer = \"adam\",\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "99/99 [==============================] - 127s 1s/step - loss: 0.5581 - acc: 0.6984 - val_loss: 0.4833 - val_acc: 0.7486\n",
      "Epoch 2/15\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.4278 - acc: 0.7953 - val_loss: 0.4824 - val_acc: 0.7971\n",
      "Epoch 3/15\n",
      "99/99 [==============================] - 125s 1s/step - loss: 0.3559 - acc: 0.8460 - val_loss: 0.4767 - val_acc: 0.7857\n",
      "Epoch 4/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.2863 - acc: 0.8824 - val_loss: 0.5053 - val_acc: 0.7886\n",
      "Epoch 5/15\n",
      "99/99 [==============================] - 128s 1s/step - loss: 0.2219 - acc: 0.9116 - val_loss: 0.5100 - val_acc: 0.7571\n",
      "Epoch 6/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.1908 - acc: 0.9205 - val_loss: 0.5021 - val_acc: 0.7886\n",
      "Epoch 7/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.1443 - acc: 0.9395 - val_loss: 0.7088 - val_acc: 0.7514\n",
      "Epoch 8/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.1257 - acc: 0.9503 - val_loss: 0.7249 - val_acc: 0.7314\n",
      "Epoch 9/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.0956 - acc: 0.9658 - val_loss: 0.5881 - val_acc: 0.7971\n",
      "Epoch 10/15\n",
      "99/99 [==============================] - 127s 1s/step - loss: 0.0808 - acc: 0.9715 - val_loss: 0.9179 - val_acc: 0.7343\n",
      "Epoch 11/15\n",
      "99/99 [==============================] - 138s 1s/step - loss: 0.0797 - acc: 0.9715 - val_loss: 1.0161 - val_acc: 0.7171\n",
      "Epoch 12/15\n",
      "99/99 [==============================] - 145s 1s/step - loss: 0.0517 - acc: 0.9832 - val_loss: 1.1624 - val_acc: 0.7457\n",
      "Epoch 13/15\n",
      "99/99 [==============================] - 127s 1s/step - loss: 0.0675 - acc: 0.9728 - val_loss: 0.9139 - val_acc: 0.7114\n",
      "Epoch 14/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.0421 - acc: 0.9854 - val_loss: 1.2871 - val_acc: 0.7629\n",
      "Epoch 15/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.0338 - acc: 0.9873 - val_loss: 1.4456 - val_acc: 0.6971\n"
     ]
    }
   ],
   "source": [
    "history3 = model3.fit(train_data,\n",
    "                    epochs=15,\n",
    "                    #batch_size = 5,\n",
    "                    validation_data = val_data\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 82s 829ms/step - loss: 0.0205 - acc: 0.9943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.020531611517071724, 0.9942965507507324]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train_3 = model3.evaluate(train_data)\n",
    "results_train_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 8s 758ms/step - loss: 1.4456 - acc: 0.6971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4455512762069702, 0.6971428394317627]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_val_3 = model3.evaluate(val_data)\n",
    "results_val_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dictionary['model 3'] =  results_val_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just for exploratory analysis - to see what does what "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing schocastic gradient descent and creating my own learning rate... using that as optimier\n",
    "#from keras.optimizers import SGD\n",
    "#opt = SGD(lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = models.Sequential()\n",
    "model4.add(layers.Conv2D(filters = 32,\n",
    "                         kernel_size=(3,3),\n",
    "                         activation='relu',\n",
    "                         input_shape = (global_target_size[0],global_target_size[0], 3)\n",
    "                        ))\n",
    "model4.add(layers.MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model4.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model4.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model4.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model4.add(layers.Flatten())\n",
    "model4.add(layers.Dense(64, activation='relu'))\n",
    "model4.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model4.compile(loss='binary_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "99/99 [==============================] - 128s 1s/step - loss: 0.5804 - acc: 0.6822 - val_loss: 0.4806 - val_acc: 0.7543\n",
      "Epoch 2/15\n",
      "99/99 [==============================] - 127s 1s/step - loss: 0.4580 - acc: 0.7852 - val_loss: 0.4494 - val_acc: 0.7714\n",
      "Epoch 3/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.3679 - acc: 0.8428 - val_loss: 0.3350 - val_acc: 0.8600\n",
      "Epoch 4/15\n",
      "99/99 [==============================] - 127s 1s/step - loss: 0.3387 - acc: 0.8517 - val_loss: 0.5627 - val_acc: 0.7400\n",
      "Epoch 5/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.2947 - acc: 0.8764 - val_loss: 0.4108 - val_acc: 0.8257\n",
      "Epoch 6/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.2767 - acc: 0.8793 - val_loss: 0.3660 - val_acc: 0.8486\n",
      "Epoch 7/15\n",
      "99/99 [==============================] - 127s 1s/step - loss: 0.2234 - acc: 0.9056 - val_loss: 0.5415 - val_acc: 0.7800\n",
      "Epoch 8/15\n",
      "99/99 [==============================] - 127s 1s/step - loss: 0.1746 - acc: 0.9268 - val_loss: 0.5410 - val_acc: 0.7943\n",
      "Epoch 9/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.1534 - acc: 0.9376 - val_loss: 0.7043 - val_acc: 0.6971\n",
      "Epoch 10/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.1197 - acc: 0.9525 - val_loss: 0.8446 - val_acc: 0.7057\n",
      "Epoch 11/15\n",
      "99/99 [==============================] - 127s 1s/step - loss: 0.0978 - acc: 0.9626 - val_loss: 0.7817 - val_acc: 0.7857\n",
      "Epoch 12/15\n",
      "99/99 [==============================] - 127s 1s/step - loss: 0.0702 - acc: 0.9756 - val_loss: 0.8990 - val_acc: 0.7486\n",
      "Epoch 13/15\n",
      "99/99 [==============================] - 128s 1s/step - loss: 0.0606 - acc: 0.9794 - val_loss: 0.9104 - val_acc: 0.7829\n",
      "Epoch 14/15\n",
      "99/99 [==============================] - 130s 1s/step - loss: 0.0460 - acc: 0.9800 - val_loss: 0.9437 - val_acc: 0.7257\n",
      "Epoch 15/15\n",
      "99/99 [==============================] - 126s 1s/step - loss: 0.0223 - acc: 0.9937 - val_loss: 1.1375 - val_acc: 0.7571\n"
     ]
    }
   ],
   "source": [
    "history4 = model4.fit(train_data,\n",
    "                    epochs=15,\n",
    "                    validation_data = val_data\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 126, 126, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 60, 60, 32)        16416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 12, 12, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 220,321\n",
      "Trainable params: 220,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 83s 834ms/step - loss: 0.0188 - acc: 0.9933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.01875309832394123, 0.9933460354804993]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train_4 = model4.evaluate(train_data)\n",
    "results_train_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 8s 754ms/step - loss: 1.1375 - acc: 0.7571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.13747239112854, 0.7571428418159485]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_val_4 = model4.evaluate(val_data)\n",
    "results_val_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dictionary['model 4'] =  results_val_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For this CNN, I am trying something different. Since my model is severely overfitting, I looked up how to reduce that and all of the articles I read said to add more data. I don't have more data readily available, so I am going to use the ImageDataGenerator to create more augmented images in hopes that this will reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2805 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Batch size here means how many \n",
    "#of the images do we want our model to be trained on\n",
    "\n",
    "# here is where I am experimenting with augmenting the data to yield reduced overfitting\n",
    "train_val_generator2 = ImageDataGenerator(rescale=1./255,\n",
    "                                          validation_split = .20,\n",
    "                                          rotation_range=15,\n",
    "                                          width_shift_range=0.1,\n",
    "                                          height_shift_range=0.1,\n",
    "                                          shear_range=0.01,\n",
    "                                          zoom_range=[0.9, 1.25],\n",
    "                                          horizontal_flip=True,\n",
    "                                          vertical_flip=False,\n",
    "                                          fill_mode='reflect',\n",
    "                                          data_format='channels_last',\n",
    "                                          brightness_range=[0.5, 1.5]\n",
    "                                         )\n",
    "\n",
    "train_data2 = train_val_generator2.flow_from_directory(train_images_dir,  \n",
    "                                                     target_size = global_target_size,\n",
    "                                                     subset='training',\n",
    "                                                     batch_size=global_batch_size,\n",
    "                                                     class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 701 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data2 = train_val_generator2.flow_from_directory(train_images_dir,\n",
    "       subset = 'validation',  \n",
    "       target_size = global_target_size,\n",
    "       batch_size=global_batch_size,\n",
    "       class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_images2, train_labels2 = next(train_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_images2, val_labels2 = next(val_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the y values to be modeled\n",
    "#y_train2 = np.asarray(train_labels2).astype('float32').reshape((-1,1))\n",
    "#y_val2 = np.asarray(val_labels2).astype('float32').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = models.Sequential()\n",
    "model3.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                         input_shape = (global_target_size[0],global_target_size[0], 3)\n",
    "                        ))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model3.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model3.add(layers.Flatten())\n",
    "model3.add(layers.Dense(64, activation='relu'))\n",
    "model3.add(layers.Dropout(0.5))\n",
    "model3.add(layers.Dense(64, activation='relu'))\n",
    "model3.add(layers.Dropout(0.5))\n",
    "model3.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.compile(loss='binary_crossentropy',\n",
    "              optimizer = \"adam\",\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "88/88 [==============================] - 139s 2s/step - loss: 0.6471 - acc: 0.6449 - val_loss: 0.6250 - val_acc: 0.6576\n",
      "Epoch 2/15\n",
      "88/88 [==============================] - 139s 2s/step - loss: 0.6061 - acc: 0.6553 - val_loss: 0.6205 - val_acc: 0.6576\n",
      "Epoch 3/15\n",
      "88/88 [==============================] - 139s 2s/step - loss: 0.5605 - acc: 0.6581 - val_loss: 0.5838 - val_acc: 0.6576\n",
      "Epoch 4/15\n",
      "88/88 [==============================] - 138s 2s/step - loss: 0.5884 - acc: 0.6553 - val_loss: 0.6247 - val_acc: 0.6576\n",
      "Epoch 5/15\n",
      "88/88 [==============================] - 139s 2s/step - loss: 0.5833 - acc: 0.6563 - val_loss: 0.5611 - val_acc: 0.6576\n",
      "Epoch 6/15\n",
      "88/88 [==============================] - 139s 2s/step - loss: 0.5276 - acc: 0.7005 - val_loss: 0.6096 - val_acc: 0.6248\n",
      "Epoch 7/15\n",
      "88/88 [==============================] - 139s 2s/step - loss: 0.5100 - acc: 0.7234 - val_loss: 0.5367 - val_acc: 0.6819\n",
      "Epoch 8/15\n",
      "88/88 [==============================] - 138s 2s/step - loss: 0.4932 - acc: 0.7736 - val_loss: 0.5904 - val_acc: 0.6334\n",
      "Epoch 9/15\n",
      "88/88 [==============================] - 139s 2s/step - loss: 0.4737 - acc: 0.7743 - val_loss: 0.5385 - val_acc: 0.7147\n",
      "Epoch 10/15\n",
      "88/88 [==============================] - 138s 2s/step - loss: 0.4515 - acc: 0.7971 - val_loss: 0.5928 - val_acc: 0.5806\n",
      "Epoch 11/15\n",
      "88/88 [==============================] - 138s 2s/step - loss: 0.4458 - acc: 0.7939 - val_loss: 0.5979 - val_acc: 0.6462\n",
      "Epoch 12/15\n",
      "88/88 [==============================] - 138s 2s/step - loss: 0.4298 - acc: 0.8061 - val_loss: 0.6494 - val_acc: 0.5435\n",
      "Epoch 13/15\n",
      "88/88 [==============================] - 141s 2s/step - loss: 0.4357 - acc: 0.8029 - val_loss: 0.5685 - val_acc: 0.6562\n",
      "Epoch 14/15\n",
      "88/88 [==============================] - 138s 2s/step - loss: 0.4255 - acc: 0.8118 - val_loss: 0.6801 - val_acc: 0.5136\n",
      "Epoch 15/15\n",
      "88/88 [==============================] - 138s 2s/step - loss: 0.4260 - acc: 0.8125 - val_loss: 0.6674 - val_acc: 0.5621\n"
     ]
    }
   ],
   "source": [
    "history3 = model3.fit(train_data2,\n",
    "                    #y_train2,\n",
    "                    epochs=15,\n",
    "                    #use_multiprocessing=True,\n",
    "                      batch_size =32,\n",
    "                    validation_data = val_data2\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_val_5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-7ab4d7b0f406>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model 5'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mresults_val_5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'results_val_5' is not defined"
     ]
    }
   ],
   "source": [
    "score_dictionary['model 5'] =  results_val_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model 1': [1.036498785018921, 0.7914285659790039],\n",
       " 'model 2': [0.8709557056427002, 0.7857142686843872],\n",
       " 'model 3': [1.4455512762069702, 0.6971428394317627],\n",
       " 'model 4': [1.13747239112854, 0.7571428418159485]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
